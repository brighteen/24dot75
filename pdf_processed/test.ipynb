{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from database_process import create_vector_store, add_documents\n",
    "import processed_documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"example_collection\"\n",
    "db_path = \"./chroma_langchain_db\" \n",
    "passage_embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large-passage\")\n",
    "\n",
    "vector_store = create_vector_store(collection_name, db_path, passage_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 문서 개수: 82\n",
      "헤더/푸터 제외 후 문서 개수: 74\n"
     ]
    }
   ],
   "source": [
    "file_path = \"data\\Don't Do RAG.pdf\"\n",
    "texts = processed_documents.main(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': 1, 'page': 1, 'category': 'heading1'}, page_content=\"<br><h1 id='1' style='font-size:20px'>Don’t Do RAG:<br>When Cache-Augmented Generation is All You Need for<br>Knowledge Tasks</h1>\"),\n",
       " Document(metadata={'id': 2, 'page': 1, 'category': 'paragraph'}, page_content=\"<br><p id='2' data-category='paragraph' style='font-size:18px'>Brian J Chan∗<br>Chao-Ting Chen∗<br>Jui-Hung Cheng∗<br>Department of Computer Science<br>National Chengchi University<br>Taipei, Taiwan<br>{110703065,110703038,110703007}@nccu.edu.tw</p>\"),\n",
       " Document(metadata={'id': 3, 'page': 1, 'category': 'paragraph'}, page_content=\"<br><p id='3' data-category='paragraph' style='font-size:18px'>Hen-Hsen Huang<br>Insititue of Information Science<br>Academia Sinica<br>Taipei, Taiwan<br>hhhuang@iis.sinica.edu.tw</p>\"),\n",
       " Document(metadata={'id': 4, 'page': 1, 'category': 'paragraph'}, page_content=\"<p id='4' data-category='paragraph' style='font-size:18px'>Abstract</p>\"),\n",
       " Document(metadata={'id': 5, 'page': 1, 'category': 'paragraph'}, page_content=\"<br><p id='5' data-category='paragraph' style='font-size:16px'>Retrieval-augmented generation (RAG) has gained traction as a<br>powerful approach for enhancing language models by integrating<br>external knowledge sources. However, RAG introduces challenges<br>such as retrieval latency, potential errors in document selection,<br>and increased system complexity. With the advent of large lan-<br>guage models (LLMs) featuring signiﬁcantly extended context win-<br>dows, this paper proposes an alternative paradigm, cache-augmented<br>generation (CAG) that bypasses real-time retrieval. Our method in-<br>volves preloading all relevant resources, especially when the docu-<br>ments or knowledge for retrieval are of a limited and manageable<br>size, into the LLM’s extended context and caching its runtime pa-<br>rameters. During inference, the model utilizes these preloaded pa-<br>rameters to answer queries without additional retrieval steps. Com-<br>parative analyses reveal that CAG eliminates retrieval latency and<br>minimizes retrieval errors while maintaining context relevance. Per-<br>formance evaluations across multiple benchmarks highlight sce-<br>narios where long-context LLMs either outperform or complement<br>traditional RAG pipelines. These ﬁndings suggest that, for certain<br>applications, particularly those with a constrained knowledge base,<br>CAG provide a streamlined and eﬃcient alternative to RAG, achiev-<br>ing comparable or superior results with reduced complexity.</p>\"),\n",
       " Document(metadata={'id': 6, 'page': 1, 'category': 'heading1'}, page_content=\"<br><h1 id='6' style='font-size:18px'>CCS Concepts</h1>\"),\n",
       " Document(metadata={'id': 7, 'page': 1, 'category': 'paragraph'}, page_content=\"<br><p id='7' data-category='paragraph' style='font-size:16px'>• Computing methodologies → Discourse, dialogue and prag-<br>matics; Natural language generation; • Information systems<br>→ Specialized information retrieval.</p>\"),\n",
       " Document(metadata={'id': 8, 'page': 1, 'category': 'paragraph'}, page_content=\"<br><p id='8' data-category='paragraph' style='font-size:18px'>Keywords</p>\"),\n",
       " Document(metadata={'id': 9, 'page': 1, 'category': 'paragraph'}, page_content=\"<br><p id='9' data-category='paragraph' style='font-size:16px'>Large Language Models, Retrieval Augmented Generation, Retrieval-<br>Free Question Answering</p>\"),\n",
       " Document(metadata={'id': 10, 'page': 1, 'category': 'heading1'}, page_content=\"<br><h1 id='10' style='font-size:18px'>1 Introduction</h1>\"),\n",
       " Document(metadata={'id': 11, 'page': 1, 'category': 'paragraph'}, page_content=\"<br><p id='11' data-category='paragraph' style='font-size:16px'>The advent of retrieval-augmented generation (RAG) [1, 3] has<br>signiﬁcantly enhanced the capabilities of large language models<br>(LLMs) by dynamically integrating external knowledge sources. RAG<br>systems have proven eﬀective in handling open-domain questions<br>and specialized tasks, leveraging retrieval pipelines to provide con-<br>textually relevant answers. However, RAG is not without its draw-<br>backs. The need for real-time retrieval introduces latency, while</p>\"),\n",
       " Document(metadata={'id': 12, 'page': 1, 'category': 'footnote'}, page_content=\"<p id='12' data-category='footnote' style='font-size:14px'>∗Three authors contributed equally to this research.</p>\"),\n",
       " Document(metadata={'id': 13, 'page': 1, 'category': 'figure'}, page_content='<br><figure id=\\'13\\'><img style=\\'font-size:14px\\' alt=\"A1\\nLLM\\nQ1\\nK1 Q1\\nK1\\nRetrieval\\nModel A2\\nQ2\\nK2\\nLLM\\nK2 Q2\\nKnowledge\\nA1 A2\\nPre-compute\\nLLM LLM LLM\\nAppend Q1 Truncate Q1\\nKnowledge Knowledge Knowledge\\nQ1 Append Q2 Q2\\nCache Cache Cache\\nQ1 Q2 Q2\" data-coord=\"top-left:(661,501); bottom-right:(1163,846)\" /></figure>'),\n",
       " Document(metadata={'id': 14, 'page': 1, 'category': 'caption'}, page_content=\"<br><caption id='14' style='font-size:16px'>Figure 1: Comparison of Traditional RAG and our CAG<br>Workﬂows: The upper section illustrates the RAG pipeline,<br>including real-time retrieval and reference text input dur-<br>ing inference, while the lower section depicts our CAG ap-<br>proach, which preloads the KV-cache, eliminating the re-<br>trieval step and reference text input at inference.</caption>\"),\n",
       " Document(metadata={'id': 15, 'page': 1, 'category': 'paragraph'}, page_content=\"<p id='15' data-category='paragraph' style='font-size:16px'>errors in selecting or ranking relevant documents can degrade the<br>quality of the generated responses. Additionally, integrating re-<br>trieval and generation components increases system complexity,<br>necessitating careful tuning and adding to the maintenance over-<br>head.</p>\"),\n",
       " Document(metadata={'id': 16, 'page': 1, 'category': 'paragraph'}, page_content=\"<br><p id='16' data-category='paragraph' style='font-size:16px'>This paper proposes an alternative paradigm, cache-augmented<br>generation (CAG), leveraging the capabilities of long-context LLMs<br>to address these challenges. Instead of relying on a retrieval pipeline,<br>as shown in Figure 1, our approach involves preloading the LLM<br>with all relevant documents in advance and precomputing the key-<br>value (KV) cache, which encapsulates the inference state of the<br>LLM. The preloaded context enables the model to provide rich,<br>contextually accurate answers without the need for additional re-<br>trieval during runtime. This approach eliminates retrieval latency,<br>mitigates retrieval errors, and simpliﬁes system architecture, all<br>while maintaining high-quality responses by ensuring the model<br>processes all relevant context holistically.</p>\"),\n",
       " Document(metadata={'id': 19, 'page': 2, 'category': 'paragraph'}, page_content=\"<p id='19' data-category='paragraph' style='font-size:18px'>Recent advances in long-context LLMs have extended their abil-<br>ity to process and reason over substantial textual inputs. By ac-<br>commodating larger context windows, these models can assimi-<br>late extensive information in a single inference step, making them<br>well-suited for tasks like document comprehension, multi-turn di-<br>alogue, and summarization of lengthy texts. This capability elimi-<br>nates the dependency on real-time retrieval, as all necessary infor-<br>mation can be preloaded into the model. These developments cre-<br>ate opportunities to streamline workﬂows for knowledge-intensive<br>tasks, potentially reducing or even eliminating the need for tradi-<br>tional RAG systems.</p>\"),\n",
       " Document(metadata={'id': 20, 'page': 2, 'category': 'paragraph'}, page_content=\"<br><p id='20' data-category='paragraph' style='font-size:18px'>Recent studies [2, 4] have investigated the performance of long-<br>context models in RAG tasks, revealing that state-of-the-art mod-<br>els like GPT-o1, GPT-4, and Claude 3.5 can eﬀectively process large<br>amounts of retrieved data, outperforming traditional systems in<br>many scenarios. Findings suggest that as long as all documents<br>ﬁt within the extended context length, traditional RAG systems<br>can be replaced by these long-context models. Similarly, Lu et al.<br>[5] has demonstrated the beneﬁts of precomputed KV caching to<br>improve eﬃciency, albeit with the need for position ID rearrange-<br>ment to enable proper functioning. Nonetheless, these methods re-<br>main vulnerable to retrieval failures inherent to RAG systems.</p>\"),\n",
       " Document(metadata={'id': 21, 'page': 2, 'category': 'paragraph'}, page_content=\"<br><p id='21' data-category='paragraph' style='font-size:18px'>Through a series of experiments comparing traditional RAG work-<br>ﬂows with our proposed approach, we identify scenarios where<br>long-context LLMs outperform RAG in both eﬃciency and accu-<br>racy. By addressing the technical and practical implications, this<br>paper aims to provide insights into when and why CAG may serve<br>as a streamlined, eﬀective alternative to RAG, particularly for cases<br>where the documents or knowledge for retrieval are of limited,<br>manageable size. Our ﬁndings challenge the default reliance on<br>RAG for knowledge integration tasks, oﬀering a simpliﬁed, robust<br>solution to harness the growing capabilities of long-context LLMs.<br>Our contributions are threefold as follows:</p>\"),\n",
       " Document(metadata={'id': 22, 'page': 2, 'category': 'list'}, page_content=\"<br><p id='22' data-category='list' style='font-size:18px'>• Retrieval-Free Long-Context Paradigm: Introduced a novel<br>approach leveraging long-context LLMs with preloaded doc-<br>uments and precomputed KV caches, eliminating retrieval<br>latency, errors, and system complexity.<br>• Performance Comparison: Conducted extensive experiments<br>showing scenarios where long-context LLMs outperform tra-<br>ditional RAG systems, especially with manageable knowl-<br>edge bases.<br>• Practical Insights: Provided actionable insights into optimiz-<br>ing knowledge-intensive workﬂows, demonstrating the via-<br>bility of retrieval-free methods for speciﬁc applications. Our<br>CAG framework is released publicly.1</p>\"),\n",
       " Document(metadata={'id': 23, 'page': 2, 'category': 'heading1'}, page_content=\"<h1 id='23' style='font-size:22px'>2 Methodology</h1>\"),\n",
       " Document(metadata={'id': 24, 'page': 2, 'category': 'paragraph'}, page_content=\"<br><p id='24' data-category='paragraph' style='font-size:18px'>Our CAG framework leverages the extended context capabilities of<br>long-context LLMs to enable retrieval-free knowledge integration.<br>By preloading external knowledge sources, such as a collection<br>of documents D = {𝑑1, 𝑑2, . . . }, and precomputing the key-value<br>(KV) cache CKV, we address the computational challenges and in-<br>eﬃciencies inherent to real-time retrieval in traditional RAG sys-<br>tems. The operation of our framework is divided into three phases:</p>\"),\n",
       " Document(metadata={'id': 25, 'page': 2, 'category': 'footnote'}, page_content=\"<p id='25' data-category='footnote' style='font-size:16px'>1https://github.com/hhhuang/CAG</p>\"),\n",
       " Document(metadata={'id': 26, 'page': 2, 'category': 'paragraph'}, page_content=\"<br><p id='26' data-category='paragraph' style='font-size:18px'>(1) External Knowledge Preloading</p>\"),\n",
       " Document(metadata={'id': 27, 'page': 2, 'category': 'paragraph'}, page_content=\"<br><p id='27' data-category='paragraph' style='font-size:18px'>In this phase, a curated collection of documents D relevant<br>to the target application is preprocessed and formatted to<br>ﬁt within the model’s extended context window. The LLM<br>M, with parameters 𝜃, processes D, transforming it into a<br>precomputed KV cache:</p>\"),\n",
       " Document(metadata={'id': 28, 'page': 2, 'category': 'paragraph'}, page_content=\"<br><p id='28' data-category='paragraph' style='font-size:20px'>CKV = KV-Encode(D) (1)</p>\"),\n",
       " Document(metadata={'id': 29, 'page': 2, 'category': 'paragraph'}, page_content=\"<br><p id='29' data-category='paragraph' style='font-size:18px'>This KV cache, which encapsulates the inference state of<br>the LLM, is stored on disk or in memory for future use. The<br>computational cost of processing D is incurred only once,<br>regardless of the number of subsequent queries.</p>\"),\n",
       " Document(metadata={'id': 30, 'page': 2, 'category': 'paragraph'}, page_content=\"<br><p id='30' data-category='paragraph' style='font-size:18px'>(2) Inference</p>\"),\n",
       " Document(metadata={'id': 31, 'page': 2, 'category': 'paragraph'}, page_content=\"<br><p id='31' data-category='paragraph' style='font-size:18px'>During inference, the precomputed KV cache CKV is loaded<br>alongside the user’s query Q. The LLM utilizes this cached<br>context to generate responses:</p>\"),\n",
       " Document(metadata={'id': 32, 'page': 2, 'category': 'equation'}, page_content=\"<br><p id='32' data-category='equation'>$${\\\\mathcal{R}}={\\\\mathcal{M}}(Q\\\\mid C_{\\\\mathrm{KV}})$$</p>\"),\n",
       " Document(metadata={'id': 33, 'page': 2, 'category': 'caption'}, page_content=\"<br><caption id='33' style='font-size:18px'>(2)</caption>\"),\n",
       " Document(metadata={'id': 34, 'page': 2, 'category': 'paragraph'}, page_content=\"<br><p id='34' data-category='paragraph' style='font-size:18px'>By preloading the external knowledge, this phase eliminates<br>retrieval latency and reduces risks of errors or omissions<br>that arise from dynamic retrieval. The combined prompt<br>P = Concat(D, Q) ensures a uniﬁed understanding of both<br>the external knowledge and the user query.</p>\"),\n",
       " Document(metadata={'id': 35, 'page': 2, 'category': 'paragraph'}, page_content=\"<p id='35' data-category='paragraph' style='font-size:18px'>To maintain system performance across multiple inference<br>sessions, the KV cache, stored in memory, can be reset eﬃ-<br>ciently. As the KV cache grows in an append-only manner<br>with new tokens 𝑡1, 𝑡2, . . . , 𝑡𝑘 sequentially appended, reset-<br>ting involves truncating these new tokens:</p>\"),\n",
       " Document(metadata={'id': 36, 'page': 2, 'category': 'equation'}, page_content=\"<br><p id='36' data-category='equation'>$$C_{\\\\mathrm{KV}}^{\\\\mathrm{resct}}=\\\\mp r\\\\mathrm{tr}\\\\mathrm{te}\\\\left(G_{\\\\mathrm{KV}},t_{1},t_{2},\\\\dots,t_{k}\\\\right)\\\\qquad\\\\qquad\\\\qquad(3)$$</p>\"),\n",
       " Document(metadata={'id': 37, 'page': 2, 'category': 'paragraph'}, page_content=\"<br><p id='37' data-category='paragraph' style='font-size:18px'>This allows for rapid reinitialization without reloading the<br>entire cache from disk, ensuring sustained speed and re-<br>sponsiveness.</p>\"),\n",
       " Document(metadata={'id': 38, 'page': 2, 'category': 'paragraph'}, page_content=\"<br><p id='38' data-category='paragraph' style='font-size:18px'>The proposed methodology oﬀers several signiﬁcant advantages<br>over traditional RAG systems:</p>\"),\n",
       " Document(metadata={'id': 39, 'page': 2, 'category': 'list'}, page_content=\"<br><p id='39' data-category='list' style='font-size:18px'>• Reduced Inference Time: By eliminating the need for real-<br>time retrieval, the inference process becomes faster and more<br>eﬃcient, enabling quicker responses to user queries.<br>• Uniﬁed Context: Preloading the entire knowledge collec-<br>tion into the LLM provides a holistic and coherent under-<br>standing of the documents, resulting in improved response<br>quality and consistency across a wide range of tasks.<br>• Simpliﬁed Architecture: By removing the need to inte-<br>grate retrievers and generators, the system becomes more<br>streamlined, reducing complexity, improving maintainabil-<br>ity, and lowering development overhead.</p>\"),\n",
       " Document(metadata={'id': 40, 'page': 2, 'category': 'paragraph'}, page_content=\"<br><p id='40' data-category='paragraph' style='font-size:18px'>Looking forward, our approach is poised to become even more<br>powerful with the anticipated advancements in LLMs. As future<br>models continue to expand their context length, they will be able<br>to process increasingly larger knowledge collections in a single in-<br>ference step. Additionally, the improved ability of these models to<br>extract and utilize relevant information from long contexts will<br>further enhance their performance. These two trends will signiﬁ-<br>cantly extend the usability of our approach, enabling it to handle<br>more complex and diverse applications. Consequently, our method-<br>ology is well-positioned to become a robust and versatile solution</p>\"),\n",
       " Document(metadata={'id': 41, 'page': 3, 'category': 'paragraph'}, page_content=\"<p id='41' data-category='paragraph' style='font-size:14px'>Don’t Do RAG:<br>When Cache-Augmented Generation is All You Need for Knowledge Tasks</p>\"),\n",
       " Document(metadata={'id': 43, 'page': 3, 'category': 'paragraph'}, page_content=\"<p id='43' data-category='paragraph' style='font-size:18px'>for knowledge-intensive tasks, leveraging the growing capabilities<br>of next-generation LLMs.</p>\"),\n",
       " Document(metadata={'id': 44, 'page': 3, 'category': 'table'}, page_content='<table id=\\'44\\' style=\\'font-size:18px\\'><tr><td>Source</td><td>Size</td><td># Docs</td><td># Tokens</td><td># QA Pairs</td></tr><tr><td rowspan=\"3\">HotPotQA</td><td>Small</td><td>16</td><td>21k</td><td>1,392</td></tr><tr><td>Medium</td><td>32</td><td>43k</td><td>1,056</td></tr><tr><td>Large</td><td>64</td><td>85k</td><td>1,344</td></tr><tr><td rowspan=\"3\">SQuAD</td><td>Small</td><td>3</td><td>21k</td><td>500</td></tr><tr><td>Medium</td><td>4</td><td>32k</td><td>500</td></tr><tr><td>Large</td><td>7</td><td>50k</td><td>500</td></tr></table>'),\n",
       " Document(metadata={'id': 45, 'page': 3, 'category': 'paragraph'}, page_content=\"<br><p id='45' data-category='paragraph' style='font-size:18px'>Table 1: Overview of the SQuAD and HotPotQA test sets<br>with varying reference text lengths, highlighting the num-<br>ber of documents, questions, and associated responses for<br>each conﬁguration.</p>\"),\n",
       " Document(metadata={'id': 46, 'page': 3, 'category': 'heading1'}, page_content=\"<h1 id='46' style='font-size:20px'>3 Experiments</h1>\"),\n",
       " Document(metadata={'id': 47, 'page': 3, 'category': 'heading1'}, page_content=\"<br><h1 id='47' style='font-size:20px'>3.1 Experimental Setup</h1>\"),\n",
       " Document(metadata={'id': 48, 'page': 3, 'category': 'paragraph'}, page_content=\"<br><p id='48' data-category='paragraph' style='font-size:18px'>To evaluate the eﬀectiveness of our proposed method, we conducted<br>experiments using two widely recognized question-answering bench-<br>marks: the Stanford Question Answering Dataset (SQuAD) 1.0 [6]<br>and the HotPotQA dataset [7]. These datasets provide complemen-<br>tary challenges, with SQuAD focusing on precise, context-aware<br>answers within single passages and HotPotQA emphasizing multi-<br>hop reasoning across multiple documents. Each of both datasets<br>consists of documents D = {𝑑1, 𝑑2, . . . } paired with questions<br>QS = {𝑞1, 𝑞2, . . . } and golden responses R = {𝑟1, 𝑟2, . . . }. These<br>datasets provide a robust platform for assessing both single-context<br>comprehension and complex multi-hop reasoning.</p>\"),\n",
       " Document(metadata={'id': 49, 'page': 3, 'category': 'paragraph'}, page_content=\"<br><p id='49' data-category='paragraph' style='font-size:18px'>To investigate how diﬀerent levels of reference text length im-<br>pact retrieval diﬃculty, we created three test sets for each dataset,<br>varying the size of the reference text. For example, in the HotPotQA-<br>small conﬁguration, we sampled 16 documents D𝑠 ⊂ D from the<br>HotPotQA document set to form a long reference text. QA pairs as-<br>sociated with D𝑠 were selected as test instances. The same method-<br>ology was applied to create test sets for SQuAD.</p>\"),\n",
       " Document(metadata={'id': 50, 'page': 3, 'category': 'paragraph'}, page_content=\"<br><p id='50' data-category='paragraph' style='font-size:18px'>The dataset statistics are summarized in Table 1. As the number<br>of documents (and hence the length of the reference text) increases,<br>the task becomes more challenging, particularly for RAG systems.<br>Longer reference texts increase the diﬃculty of accurately retriev-<br>ing the correct information, which is crucial for LLMs to generate<br>high-quality responses.</p>\"),\n",
       " Document(metadata={'id': 51, 'page': 3, 'category': 'paragraph'}, page_content=\"<br><p id='51' data-category='paragraph' style='font-size:18px'>The primary task involves generating accurate and contextually<br>relevant answers R ˆ = { ˆ𝑟1, ˆ𝑟2, . . . } for the SQuAD and HotPotQA<br>questions, based on the respective preloaded passages. By leverag-<br>ing the precomputed key-value cache CKV = KV-Encode(D), our<br>system generates responses ˆ𝑟𝑖 = M (𝑞𝑖 | CKV) without relying on<br>retrieval mechanisms during inference. This uniﬁed approach al-<br>lows for direct performance comparisons against traditional RAG<br>systems, highlighting the strengths and limitations of our method<br>across diverse QA challenges.</p>\"),\n",
       " Document(metadata={'id': 52, 'page': 3, 'category': 'paragraph'}, page_content=\"<br><p id='52' data-category='paragraph' style='font-size:18px'>The experiments were executed on Tesla V100 32G × 8 GPUs.<br>For all experiments, we used the Llama 3.1 8B Instruction model<br>as the underlying LLM across all systems, including both the RAG</p>\"),\n",
       " Document(metadata={'id': 53, 'page': 3, 'category': 'paragraph'}, page_content=\"<br><p id='53' data-category='paragraph' style='font-size:18px'>baselines and our proposed method. This model supports input<br>sizes of up to 128k tokens, enabling the processing of extensive<br>contexts. For our proposed method, the context of each dataset<br>was preloaded into the model via a precomputed key-value (KV)<br>cache. For SQuAD, the documents DS were encoded into a KV<br>cache CS = KV-Encode (DS), while for HotPotQA, the documents<br>KV<br>DH were encoded into CH = KV-Encode(DH). These caches were<br>KV<br>stored oﬄine and loaded during inference to eliminate the need for<br>real-time retrieval, ensuring comprehensive access to all relevant<br>information for each dataset.</p>\"),\n",
       " Document(metadata={'id': 54, 'page': 3, 'category': 'paragraph'}, page_content=\"<p id='54' data-category='paragraph' style='font-size:20px'>3.2 Baseline Systems</p>\"),\n",
       " Document(metadata={'id': 55, 'page': 3, 'category': 'paragraph'}, page_content=\"<br><p id='55' data-category='paragraph' style='font-size:18px'>The baseline RAG systems were implemented using the LlamaIn-<br>dex framework,2 employing two retrieval strategies: BM25 for sparse<br>retrieval and OpenAI Indexes for dense retrieval. Each dataset—SQuAD<br>and HotPotQA—was evaluated separately, with retrieval systems<br>conﬁgured to fetch passages exclusively from the respective dataset<br>to ensure focused and fair evaluation. The details of each baseline<br>system are as follows:</p>\"),\n",
       " Document(metadata={'id': 56, 'page': 3, 'category': 'paragraph'}, page_content=\"<br><p id='56' data-category='paragraph' style='font-size:18px'>(1) Sparse Retrieval System (BM25): The ﬁrst baseline sys-<br>tem employed BM25 indexes for retrieval. BM25, a sparse re-<br>trieval algorithm, ranks documents based on term frequency-<br>inverse document frequency (TF-IDF) and document length<br>normalization. Given a query 𝑞𝑖, BM25 retrieves the top-𝑘<br>passages P𝑘 = {𝑝1, 𝑝2, . . . , 𝑝𝑘 } from the indexed collection<br>D. These passages were then passed to the generator, M,<br>to synthesize answers:</p>\"),\n",
       " Document(metadata={'id': 57, 'page': 3, 'category': 'equation'}, page_content=\"<br><p id='57' data-category='equation'>$$\\\\dot{\\\\tau}_{i}=\\\\mathcal{N}(q_{i}\\\\mid\\\\mathcal{P}_{k})$$</p>\"),\n",
       " Document(metadata={'id': 58, 'page': 3, 'category': 'caption'}, page_content=\"<br><caption id='58' style='font-size:18px'>(4)</caption>\"),\n",
       " Document(metadata={'id': 59, 'page': 3, 'category': 'list'}, page_content=\"<br><p id='59' data-category='list' style='font-size:18px'>BM25 provides a robust and interpretable retrieval mecha-<br>nism, suited for tasks involving keyword matching.<br>(2) Dense Retrieval System (OpenAI Indexes) The second<br>baseline utilized OpenAI indexes,3 which employ dense em-<br>beddings to represent both documents and queries in a shared<br>semantic space. For a query 𝑞𝑖, dense retrieval selects the<br>top-𝑘 passages P𝑘 that semantically align with the query,<br>oﬀering improved contextual understanding compared to<br>sparse methods. These passages were similarly passed to<br>the generator for answer synthesis as Equation 4. This sys-<br>tem is particularly eﬀective for questions requiring nuanced<br>contextual matching beyond exact term overlap.</p>\"),\n",
       " Document(metadata={'id': 60, 'page': 3, 'category': 'paragraph'}, page_content=\"<br><p id='60' data-category='paragraph' style='font-size:18px'>Our experiments were conducted on both the SQuAD and Hot-<br>PotQA datasets to evaluate the performance of diﬀerent systems<br>in terms of similarity to ground-truth answers, measured using<br>BERTScore [8]. For the RAG baselines, the top-1, top-3, top-5, and<br>top-10 retrieved passages were used for inference. In contrast, our<br>CAG utilized the preloaded context speciﬁc to each dataset to gen-<br>erate answers without retrieval constraints.</p>\"),\n",
       " Document(metadata={'id': 61, 'page': 3, 'category': 'paragraph'}, page_content=\"<p id='61' data-category='paragraph' style='font-size:20px'>3.3 Results</p>\"),\n",
       " Document(metadata={'id': 62, 'page': 3, 'category': 'paragraph'}, page_content=\"<br><p id='62' data-category='paragraph' style='font-size:18px'>As shown in Table 2, the experimental results revealed clear distinc-<br>tions between our proposed method and traditional RAG systems.<br>Our proposed approach achieved the highest BERTScore in most</p>\"),\n",
       " Document(metadata={'id': 63, 'page': 3, 'category': 'footnote'}, page_content=\"<p id='63' data-category='footnote' style='font-size:16px'>2https://www.llamaindex.ai/framework<br>3https://cookbook.openai.com/examples/evaluation/evaluate_rag_with_llamaindex</p>\"),\n",
       " Document(metadata={'id': 66, 'page': 4, 'category': 'caption'}, page_content=\"<caption id='66' style='font-size:16px'>Table 2: Experimental Results</caption>\"),\n",
       " Document(metadata={'id': 67, 'page': 4, 'category': 'table'}, page_content='<table id=\\'67\\' style=\\'font-size:16px\\'><tr><td>Size</td><td>System</td><td>Top-𝑘</td><td>HotPotQA BERT-Score</td><td>SQuAD BERT-Score</td></tr><tr><td rowspan=\"9\">Small</td><td rowspan=\"4\">Sparse RAG</td><td>1</td><td>0.0673</td><td>0.7469</td></tr><tr><td>3</td><td>0.0673</td><td>0.7999</td></tr><tr><td>5</td><td>0.7549</td><td>0.8022</td></tr><tr><td>10</td><td>0.7461</td><td>0.8191</td></tr><tr><td rowspan=\"4\">Dense RAG</td><td>1</td><td>0.7079</td><td>0.6445</td></tr><tr><td>3</td><td>0.7509</td><td>0.7304</td></tr><tr><td>5</td><td>0.7414</td><td>0.7583</td></tr><tr><td>10</td><td>0.7516</td><td>0.8035</td></tr><tr><td>CAG (Ours)</td><td></td><td>0.7759</td><td>0.8265</td></tr><tr><td rowspan=\"9\">Medium</td><td rowspan=\"4\">Sparse RAG</td><td>1</td><td>0.6652</td><td>0.7036</td></tr><tr><td>3</td><td>0.7619</td><td>0.7471</td></tr><tr><td>5</td><td>0.7616</td><td>0.7467</td></tr><tr><td>10</td><td>0.7238</td><td>0.7420</td></tr><tr><td rowspan=\"4\">Dense RAG</td><td>1</td><td>0.7135</td><td>0.6188</td></tr><tr><td>3</td><td>0.7464</td><td>0.6869</td></tr><tr><td>5</td><td>0.7278</td><td>0.7047</td></tr><tr><td>10</td><td>0.7451</td><td>0.7350</td></tr><tr><td>CAG (Ours)</td><td></td><td>0.7696</td><td>0.7512</td></tr><tr><td rowspan=\"9\">Large</td><td rowspan=\"4\">Sparse RAG</td><td>1</td><td>0.6567</td><td>0.7135</td></tr><tr><td>3</td><td>0.7424</td><td>0.7510</td></tr><tr><td>5</td><td>0.7495</td><td>0.7543</td></tr><tr><td>10</td><td>0.7358</td><td>0.7548</td></tr><tr><td rowspan=\"4\">Dense RAG</td><td>1</td><td>0.6969</td><td>0.6057</td></tr><tr><td>3</td><td>0.7426</td><td>0.6908</td></tr><tr><td>5</td><td>0.7300</td><td>0.7169</td></tr><tr><td>10</td><td>0.7398</td><td>0.7499</td></tr><tr><td>CAG (Ours)</td><td></td><td>0.7527</td><td>0.7640</td></tr></table>'),\n",
       " Document(metadata={'id': 68, 'page': 4, 'category': 'caption'}, page_content=\"<caption id='68' style='font-size:16px'>Table 3: Comparison of Generation Time</caption>\"),\n",
       " Document(metadata={'id': 69, 'page': 4, 'category': 'table'}, page_content='<table id=\\'69\\' style=\\'font-size:16px\\'><tr><td>Dataset</td><td>Size</td><td>System</td><td>Generation Time (s)</td></tr><tr><td rowspan=\"6\">HotpotQA</td><td rowspan=\"2\">Small</td><td>CAG</td><td>0.85292</td></tr><tr><td>w/o CAG</td><td>9.24734</td></tr><tr><td rowspan=\"2\">Medium</td><td>CAG</td><td>1.66132</td></tr><tr><td>w/o CAG</td><td>28.81642</td></tr><tr><td rowspan=\"2\">Large</td><td>CAG</td><td>2.32667</td></tr><tr><td>w/o CAG</td><td>94.34917</td></tr><tr><td rowspan=\"6\">SQuAD</td><td rowspan=\"2\">Small</td><td>CAG</td><td>1.06509</td></tr><tr><td>w/o CAG</td><td>10.29533</td></tr><tr><td rowspan=\"2\">Medium</td><td>CAG</td><td>1.73114</td></tr><tr><td>w/o CAG</td><td>13.35784</td></tr><tr><td rowspan=\"2\">Large</td><td>CAG</td><td>2.40577</td></tr><tr><td>w/o CAG</td><td>31.08368</td></tr></table>'),\n",
       " Document(metadata={'id': 70, 'page': 4, 'category': 'paragraph'}, page_content=\"<p id='70' data-category='paragraph' style='font-size:16px'>situations, outperforming both RAG systems. By preloading the en-<br>tire context from the test set, our system eliminates retrieval errors<br>and ensures holistic reasoning over all relevant information. This<br>advantage is particularly evident in scenarios where RAG systems</p>\"),\n",
       " Document(metadata={'id': 71, 'page': 4, 'category': 'paragraph'}, page_content=\"<br><p id='71' data-category='paragraph' style='font-size:16px'>might retrieve incomplete or irrelevant passages, leading to subop-<br>timal answer generation. These results underscore the robustness<br>and eﬃciency of our method, especially for tasks requiring a uni-<br>ﬁed understanding of the source material. While dense retrieval<br>methods such as OpenAI Indexes perform better than sparse re-<br>trieval methods like BM25, both are inherently limited by their<br>dependence on retrieval accuracy and ranking heuristics. Our ap-<br>proach bypasses these challenges, leveraging the long-context ca-<br>pabilities of the Llama 3.1 model to achieve superior performance.</p>\"),\n",
       " Document(metadata={'id': 72, 'page': 4, 'category': 'paragraph'}, page_content=\"<br><p id='72' data-category='paragraph' style='font-size:16px'>Table 3 compares our CAG approach with standard in-context<br>learning, where the reference text is provided dynamically dur-<br>ing inference, requiring real-time KV-cache computation. The re-<br>sults demonstrate that CAG dramatically reduces generation time,<br>particularly as the reference text length increases. This eﬃciency<br>stems from preloading the KV-cache, which eliminates the need to<br>process the reference text on the ﬂy.</p>\"),\n",
       " Document(metadata={'id': 73, 'page': 4, 'category': 'paragraph'}, page_content=\"<br><p id='73' data-category='paragraph' style='font-size:16px'>Moreover, CAG is also faster than traditional RAG systems, as<br>it bypasses the retrieval stage entirely. Unlike RAG, CAG does not<br>require retrieval or reference text input during inference, stream-<br>lining the process and further enhancing eﬃciency. These advan-<br>tages make CAG an optimal solution for scenarios with extensive<br>reference contexts, oﬀering substantial time savings without com-<br>promising performance.</p>\"),\n",
       " Document(metadata={'id': 74, 'page': 4, 'category': 'paragraph'}, page_content=\"<p id='74' data-category='paragraph' style='font-size:20px'>4 Conclusion</p>\"),\n",
       " Document(metadata={'id': 75, 'page': 4, 'category': 'paragraph'}, page_content=\"<br><p id='75' data-category='paragraph' style='font-size:16px'>As long-context LLMs evolve, we present a compelling case for<br>rethinking traditional RAG workﬂows. While our work empha-<br>sizes eliminating retrieval latency, there is potential for hybrid ap-<br>proaches that combine preloading with selective retrieval. For ex-<br>ample, a system could preload a foundation context and use re-<br>trieval only to augment edge cases or highly speciﬁc queries. This<br>would balance the eﬃciency of preloading with the ﬂexibility of<br>retrieval, making it suitable for scenarios where context complete-<br>ness and adaptability are equally important.</p>\"),\n",
       " Document(metadata={'id': 76, 'page': 4, 'category': 'paragraph'}, page_content=\"<br><p id='76' data-category='paragraph' style='font-size:20px'>References</p>\"),\n",
       " Document(metadata={'id': 77, 'page': 4, 'category': 'list'}, page_content=\"<br><p id='77' data-category='list' style='font-size:14px'>[1] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai,<br>Jiawei Sun, and Haofen Wang. 2023. Retrieval-augmented generation for large<br>language models: A survey. arXiv preprint arXiv:2312.10997 (2023).<br>[2] Quinn Leng, Jacob Portes, Sam Havens, Matei Zaharia, and Michael Carbin. 2024.<br>Long Context RAG Performance of Large Language Models. arXiv preprint<br>arXiv:2411.03538 (2024).<br>[3] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir<br>Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-<br>täschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp<br>tasks. Advances in Neural Information Processing Systems 33 (2020), 9459–9474.<br>[4] Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Ben-<br>dersky. 2024. Retrieval Augmented Generation or Long-Context LLMs?<br>A Comprehensive Study and Hybrid Approach. In Proceedings of the 2024<br>Conference on Empirical Methods in Natural Language Processing: Industry<br>Track, Franck Dernoncourt, Daniel Preoţiuc-Pietro, and Anastasia Shimorina<br>(Eds.). Association for Computational Linguistics, Miami, Florida, US, 881–893.<br>https://doi.org/10.18653/v1/2024.emnlp-industry.66<br>[5] Songshuo Lu, Hua Wang, Yutian Rong, Zhi Chen, and Yaohua Tang.<br>2024. TurboRAG: Accelerating Retrieval-Augmented Generation with Pre-<br>computed KV Caches for Chunked Text. arXiv:2410.07590 [cs.CV]<br>https://arxiv.org/abs/2410.07590<br>[6] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.<br>SQuAD: 100,000+ Questions for Machine Comprehension of Text. In Proceedings<br>of the 2016 Conference on Empirical Methods in Natural Language Processing, Jian<br>Su, Kevin Duh, and Xavier Carreras (Eds.). Association for Computational Lin-<br>guistics, Austin, Texas, 2383–2392. https://doi.org/10.18653/v1/D16-1264<br>[7] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Rus-<br>lan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A Dataset for</p>\"),\n",
       " Document(metadata={'id': 80, 'page': 5, 'category': 'paragraph'}, page_content=\"<p id='80' data-category='paragraph' style='font-size:14px'>Diverse, Explainable Multi-hop Question Answering. In Conference on Empirical<br>Methods in Natural Language Processing (EMNLP).</p>\"),\n",
       " Document(metadata={'id': 81, 'page': 5, 'category': 'paragraph'}, page_content=\"<br><p id='81' data-category='paragraph' style='font-size:14px'>[8] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi.<br>[n. d.]. BERTScore: Evaluating Text Generation with BERT. In International Con-<br>ference on Learning Representations.</p>\")]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1334\n"
     ]
    }
   ],
   "source": [
    "vector_store = add_documents(vector_store, texts)\n",
    "\n",
    "all_data = vector_store.get()\n",
    "print(len(all_data['ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 문서 개수: 155\n",
      "헤더/푸터 제외 후 문서 개수: 136\n"
     ]
    }
   ],
   "source": [
    "file_path = \"data\\Retrieval-Augmented Generation for.pdf\"\n",
    "texts = processed_documents.main(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1470\n"
     ]
    }
   ],
   "source": [
    "vector_store = add_documents(vector_store, texts)\n",
    "\n",
    "all_data = vector_store.get()\n",
    "print(len(all_data['ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[디버그] db : <langchain_chroma.vectorstores.Chroma object at 0x00000113DAE6AFD0>\n",
      "[디버그] db : <langchain_chroma.vectorstores.Chroma object at 0x00000113DAE6AFD0>\n",
      "[디버그] db : <langchain_chroma.vectorstores.Chroma object at 0x00000113DAE6AFD0>\n",
      "[디버그] db : <langchain_chroma.vectorstores.Chroma object at 0x00000113DAE6AFD0>\n",
      "[디버그] db : <langchain_chroma.vectorstores.Chroma object at 0x00000113DAE6AFD0>\n",
      "[디버그] db : <langchain_chroma.vectorstores.Chroma object at 0x00000113DAE6AFD0>\n",
      "[디버그] db : <langchain_chroma.vectorstores.Chroma object at 0x00000113DAE6AFD0>\n",
      "[디버그] db : <langchain_chroma.vectorstores.Chroma object at 0x00000113DAE6AFD0>\n",
      "[디버그] db : <langchain_chroma.vectorstores.Chroma object at 0x00000113DAE6AFD0>\n",
      "[디버그] db : <langchain_chroma.vectorstores.Chroma object at 0x00000113DAE6AFD0>\n"
     ]
    }
   ],
   "source": [
    "from llm_process import generate_response\n",
    "response = generate_response(vector_store, \"Retrieval-Augmented Generation for\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'question_summary': {'summary': 'Retrieval-Augmented Generation에 대한 질문'},\n",
       "  'answer': {'main_content': ['Retrieval-Augmented Generation은 지식 집약적인 자연어 처리 작업을 위한 방법론입니다.'],\n",
       "   'metadata': [{'chunk_id': 0, 'page_number': 1, 'chunk_length': 1}]},\n",
       "  'conclusion': {'conclusion': 'Retrieval-Augmented Generation은 자연어 처리 작업에서 필요한 지식을 효과적으로 활용하기 위한 방법입니다. 이 방법은 대량의 데이터베이스에서 관련 정보를 검색하여 생성 모델에 통합함으로써, 더 정확하고 풍부한 정보를 제공할 수 있습니다. 이는 특히 지식이 많이 요구되는 작업에서 유용하며, 정보 검색과 생성의 결합을 통해 더 나은 성능을 발휘할 수 있습니다.'}},\n",
       " {'next': False,\n",
       "  'score': 0.7,\n",
       "  'new_query': 'Retrieval-Augmented Generation이 무엇인지, 그리고 이 방법이 자연어 처리 작업에서 어떻게 사용되는지 설명해 주세요.',\n",
       "  'reason': 'LLM의 답변은 Retrieval-Augmented Generation이 지식 집약적인 자연어 처리 작업에 사용된다는 것을 설명하고 있지만, 문서의 구체적인 내용과는 직접적으로 연결되지 않습니다. 문서의 제목만을 기반으로 한 답변이므로, 문서의 구체적인 내용을 반영한 답변이 필요합니다. 따라서, 질문을 구체적으로 재작성하여 LLM이 문서의 내용을 더 잘 반영할 수 있도록 해야 합니다.'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bitcomputer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
